[[CH-Scheduling]]
== 调度

要完全理解在 ERTS 系统中时间花在何处，您需要理解系统如何决定运行哪个 Erlang 代码以及何时运行它。这些决定是由调度器做出的。

调度程序负责系统的实时性保证。在计算机科学对 “*实时*” 一词的严格定义中，实时系统必须能够保证在指定的时间内作出响应。也就是说，有真正的截止日期，每个任务都必须在最终期限之前完成。在 Erlang 中没有这样的保证，只保证超时*不会* 在给定的最终期限 *之前* 触发。

在像 Erlang 这样的通用系统中，我们希望能够处理各种程序和负载，所以调度器将不得不做出一些妥协。总会有一些极端情况，在这些情况下，通用的调度器的行为会变得很糟糕。阅读完本章后，您将对 Erlang 调度器的工作方式有更深的理解，特别是当它可能不在最佳状态工作时。你应该能够设计你的系统以避免极端情况，还应该能够分析行为不正常的系统。

=== 并发、并行，抢占式多任务

Erlang 是一种并发语言。当我们说进程并发运行时，我们的意思是：对于一个外部观察者来说，它看起来像是两个（译注：或多个）进程同时在执行。在单核系统中，这是通过抢占式多任务实现的。这意味着一个进程将运行一段时间，然后虚拟机的调度器将挂起它，让另一个进程运行。

在多核或分布式系统中，我们可以实现真正的并行性，即两个或多个进程实际上同时执行。在启用SMP的仿真器中，系统使用几个操作系统线程来间接地执行Erlang进程，每个线程运行一个调度程序和仿真器。在使用ERTS默认设置的系统中，每个启用的核心 (物理核心或超线程) 将有一个线程。

通过检查是否启用了 SMP 支持，我们可以检查我们有一个能够支持并行执行的系统：

----
iex(1)> :erlang.system_info :smp_support
true
----

We can also check how many schedulers we have running in the
system:

----
iex(2)> :erlang.system_info :schedulers_online
4
----
（译注：上边的两个例子使用了 Elixir Shell，其实在 Erlang Shell 操作应该更简单直接，在译者机器上的运行情况如下：）
----
bash> erl

Erlang/OTP 23 [erts-11.0.3] [source] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:1] [hipe] [dtrace]

Eshell V11.0.3  (abort with ^G)
1> erlang:system_info(smp_support).
true
2> erlang:system_info(schedulers_online).
8
----

我们可以在 Observer 中看到如下图所示的信息。

如果我们生成了比现有的调度器更多的进程，并让它们做一些繁忙的工作，我们可以看到有许多进程在并行运行 ( _running_ )，还有一些进程是可运行 ( _runnable_ ) 的，但目前没有运行。我们可以通过函数 `erlang:process_info/2` 看到这一点。

----

1> Loop = fun (0, _) -> ok; (N, F) -> F(N-1, F) end,
   BusyFun = fun() -> spawn(fun () -> Loop(1000000, Loop) end) end,
   SpawnThem = fun(N) -> [ BusyFun() || _ <- lists:seq(1, N)] end,
   GetStatus = fun() -> lists:sort([{erlang:process_info(P, [status]), P}
                        || P <- erlang:processes()]) end,
   RunThem = fun (N) -> SpawnThem(N), GetStatus() end,
   RunThem(8).

[{[{status,garbage_collecting}],<0.62.0>},
 {[{status,garbage_collecting}],<0.66.0>},
 {[{status,runnable}],<0.60.0>},
 {[{status,runnable}],<0.61.0>},
 {[{status,runnable}],<0.63.0>},
 {[{status,runnable}],<0.65.0>},
 {[{status,runnable}],<0.67.0>},
 {[{status,running}],<0.58.0>},
 {[{status,running}],<0.64.0>},
 {[{status,waiting}],<0.0.0>},
 {[{status,waiting}],<0.1.0>},

...
----

在本章的后面，我们将进一步研究进程可能具有的各种状态，但现在我们需要知道的是正在运行或垃圾收集 ( _running_ 或 _garbage_collecting_ ) 的进程，实际上正在调度器中运行。由于示例中的机器有 4 个核和 4 个调度器，因此有 4 个进程并行运行 ( shell 进程和 3 个繁忙的进程 )。还有 5 个忙碌的进程以 runnable 状态等待运行。

通过使用 Observer 中的 _Load Charts_ 选项卡，我们可以看到在繁忙的进程执行时，所有 4 个调度器都已满负载。

----
observer:start().
ok

3> RunThem(8).
----

image::../images/observer_load.jpg[Observer]

=== 在 C 层面上的协作的 ERTS 抢占式多任务

Erlang 级别的抢占式多任务，是通过 C 语言级别的协作多任务来实现的。Erlang 语言、编译器和虚拟机一起工作，以确保 Erlang 进程的执行将在有限的时间内完成，并让下一个进程运行。用于测量和限制允许执行时间的技术称为规约值计数，我们接下来将看到有关规约值计数的所有细节。

=== 规约值

人们可以将 BEAM 中的调度描述为：在协同调度之上的抢占式调度。进程只能在执行的某些时刻被挂起，例如在 receive 或函数调用时。这样，调度是协作的---进程必须执行允许挂起的代码。Erlang 代码的特性使得进程在不执行函数调用时，几乎不可能长时间运行。有一些内建函数 ( BIFs ) 仍然可能花费很长时间而不会让步。另外，如果调用实现糟糕的本地实现函数 (NIF, Native Implemented Function) 中的 C 代码，也可能会长时间阻塞一个调度器。我们将在 xref:CH-C[] 中看到如何编写表现良好的NIFs。

由于除了递归和列表解析式 ( list comprehension ) 之外没有其他的循环结构，因此不进行函数调用就不可能永远循环。每个函数调用都被算作一次 `规约`；当进程的规约值减少达到下限时，它将被挂起。

[VERSION]
.版本信息
====
在 OTP-20.0之前的版本，`CONTEXT_REDS` 的值曾经被设置为 2000.
====

[NOTE]
.规约 ( Reductions )
====
规约这个术语来自 Erlang 的祖先 Prolog。在 Prolog 中，每个执行步骤都是一个目标规约 (goal-reduction)，每个步骤都将一个逻辑问题简化为它的组成部分，然后尝试解决每个部分。
====

==== 你能得到多少规约值？

当进程被调度时，它将获得 `CONTEXT_REDS` (在 https://github.com/erlang/otp/blob/OTP-23.1/erts/emulator/beam/erl_vm.h#L39[erl_vm.h] 中定义，当前值为4000) 定义的一个规约值。在用尽规约值，执行 receive 并且在收件箱中没有匹配的消息时，该进程将被挂起，另一个进程将被调度。

如果 VM 已经执行了 `INPUT_REDUCTIONS` 定义的规约次数(当前是 `2*CONTEXT_REDS`，也在 +erl_vm.h+ 中定义)，或者没有进程 ready，调度器将执行系统级活动。也就是检查IO；我们稍后会详细了解。

==== 规约值究竟是啥？

规约是什么还没有完全定义，但至少每个函数调用都应该算作规约。当谈到 BIFs 和 NIFs 时，事情变得有点复杂。如果不使用规约值和让步 ( yield )，进程不应该能够运行“很长时间”。用 C 编写的函数不能在中间产生让步，它必须确保它处于干净的状态并返回。为了可重入，它必须在返回之前保存它的内部状态，然后在再次进入时再次设置状态。这样做的代价会非常大，特别是对于一个有时只做少量工作，有时做大量工作的函数来说。用 C 而不是 Erlang 编写函数的原因通常是为了获得性能收益，并且不需要做不必要的簿记工作。由于除了 Erlang 级别上的函数调用之外，对于什么是一次 规约 没有明确的定义，因此存在这样的风险：用 C 实现的函数在每次规约时比普通的 Erlang 函数花费更多的时钟周期。这可能会导致调度器不平衡，甚至导致资源饥饿。

例如，在 R16 之前的 Erlang 版本中，BIFs 的 `binary_to_term/1` 和 `term_to_binary/1` 是不让步的，并且只算一次规约。这意味着以特大项式 (为参数) 调用这些函数的进程可能会饿死其他进程。因为调度器之间的进程平衡方式，这种情况甚至可能发生在 SMP 系统中，我们很快就会讲到。

当进程运行时，仿真器在 (寄存器映射) 变量 `FCALLS` 中保留要执行的规约数 (参见 +beam_emu.c+)。

在 Elixir 中，我们可以用  `hipe_bifs:show_pcb/1` 检查这个值：

----
iex(13)> :hipe_bifs.show_pcb self
 P: 0x00007efd7c2c0400
 -----------------------------------------------------------------
 Offset| Name          |              Value |             *Value |
     0 | id            | 0x00000270000004e3 |                    |

 ...

   328 | rcount        | 0x0000000000000000 |                    |
   336 | reds          | 0x000000000000a528 |                    |

 ...

   320 | fcalls        | 0x00000000000004a3 |                    |
----

[NOTE]

译注：如果以上命令无法执行，可以使用 erlang:process_info(self()). 查看 reductions 的值

`reds` 字段会追踪进程在最后一次挂起之前所完成的规约总数。通过监视这个数字，您可以看到哪些进程做了最多的工作。

你可以通过调用 `erlang:process_info/2` 并将第二个参数设置为 reductions 原子，来查看进程的规约值总数。你还可以在 observer 的 process 选项卡中，或在 Erlang shell 中的 i/0 命令中看到这个数字。

如前所述，每当进程启动时，字段 `fcalls` 被设置为 `CONTEXT_REDS` 的值，并且进程执行每个函数调用的时候， `fcalls` 将减少1。当进程被挂起时，reds 字段会随着执行的减少数量而增加。用类似 C 的代码描述，类似： `p -> reds += (CONTEXT_REDS - p -> fcalls)`。

通常进程会执行所有分配的规约数，此时 `fcalls` 为0，但是如果进程在 receive 中挂起等待消息，那么它还会留下一些规约数未用尽。

当一个进程用尽它的所有规约数，他会让步给另一个进程运行，这时，它将从进程状态 _running_ 变为状态 _runnalbe_ ，如果它在执行 receive 时让步，它将进入 _waiting_ 状态(等待消息)。在下一节中，我们将查看进程可能处于的所有不同状态。

=== 进程状态

The field `status` in the PCB contains the process state. It can be one
of _free_, _runnable_, _waiting_, _running_, _exiting_, _garbing_,
and _suspended_. When a process exits it is marked as
free---you should never be able to see a process in this state,
it is a short lived state where the process no longer exist as
far as the rest of the system is concerned but there is still
some clean up to be done (freeing memory and other resources).

Each process status represents a state in the Process State
Machine. Events such as a timeout or a delivered
message triggers transitions along the edges in the state machine.
The _Process State Machine_ looks like this:

[[process_state_machine]]
.Process State Machine
[ditaa]
----

                                +--------+
                                |  free  |
              +-----------+     |        |
          +---> suspended |     +---^----+
          | +-+           |         |
          | | ++-------^^-+     +---+----+
          | |  |       ||       | exiting|
          | |  |       ||       |        |
          | |  |       ||       +---^----+
          | |  |       ||suspend    |
          | |  |       |+--------+  |
          | |  | resume|         |  | exit
          | |  |       |         |  |
          | | +v-------+--+    +-+--+-----+   GC   +----------+
          | | | runnable  |+-->| running  +--------> garbing  |
          | | |           |    |          <--------+          |
          | | +^------^---+    +----+-----+        +----------+
          | |  |      |             |
          | |  | msg  | timeout     | receive
          | |  |      |             |
          | |  |      |             |
          | |  |      |        +----v-----+
          | |  |      +--------+ waiting  |
          | |  +---------------+          |
          | |                  +^---+-----+
          | |resume             |   |
          | +-------------------+   |suspend
          +-------------------------+

----

The normal states for a process are _runnable_, _waiting_, and _running_.
A running process is currently executing code in one of the schedulers.
When a process enters a receive and there is no matching message in
the message queue, the process will become waiting until a message
arrives or a timeout occurs. If a process uses up all its reductions,
it will become runnable and wait for a scheduler to pick it up again.
A waiting process receiving a message or a timeout will become
runnable.


Whenever a process needs to do garbage collection, it will go into
the _garbing_
state until the GC is done. While it is doing GC
it saves the old state in the field `gcstatus` and when it is done
it sets the state back to the old state using `gcstatus`.

The suspended state is only supposed to be used for debugging
purposes. You can call `erlang:suspend_process/2` on another process
to force it into the suspended state. Each time a process calls
`suspend_process` on another process, the _suspend count_ is increased.
This is recorded in the field `rcount`.
A call to (`erlang:resume_process/1`) by the suspending process will
decrease the suspend count. A process in the suspend state will not
leave the suspend state until the suspend count reaches zero.

The field `rstatus` (resume status) is used to keep track of the
state the process was in before a suspend. If it was _running_
or _runnable_ it will start up as _runnable_, and if it was _waiting_
it will go back to the wait queue. If a suspended waiting process
receives a timeout `rstatus` is set to _runnable_ so it will resume
as _runnable_.

To keep track of which process to run next the scheduler keeps
the processes in a queue.







=== 进程队列
The main job of the scheduler is to keep track of work queues,
that is, queues of processes and ports.

There are two process states that the scheduler has to handle,
_runnable_, and _waiting_.
Processes waiting to receive a message are in
the waiting state. When a waiting process receives a message the send
operations triggers a move of the receiving process into the runnable
state. If the receive statement has a timeout the scheduler has to
trigger the state transition to runnable when the timeout triggers.
We will cover this mechanism later in this chapter.

==== Ready 队列
Processes in the runnable state are placed in a FIFO (first in first
out) queue handled by the scheduler, called the _ready queue_. The
queue is implemented by a first and a last pointer and by the next
pointer in the PCB of each participating process.
When a new process is added to the queue the
_last_ pointer is followed and the process is added to the end of the
queue in an O(1) operation. When a new process is scheduled it is
just popped from the head (the _first_ pointer) of the queue.

[[the_ready_queue]]
----
 The Ready Queue

 First: -->  P5       +---> P3       +-+-> P17
             next: ---+     next: ---+ |  next: NULL
                                       |
 Last: --------------------------------+
----

In a SMP system, where you have several scheduler threads,
there is one queue per scheduler.

[[the_smp_ready_queues]]
----
 Scheduler 1       Scheduler 2      Scheduler 3      Scheduler 4

 Ready: P5         Ready: P1        Ready: P7        Ready: P9
        P3                P4               P12
        P17                                P10

----

The reality is slightly more complicated since Erlang processes have
priorities. Each scheduler actually has three queues. One queue for
_max priority_ tasks, one for _high priority_ tasks and one queue
containing both _normal_ and _low priority_ tasks.

[[priority_ready_queues]]
----
 Scheduler 1       Scheduler 2      Scheduler 3      Scheduler 4

 Max:    P5        Max:             Max:             Max:
 High:             High:  P1        High:            High:
 Normal: P3        Ready: P4        Ready: P7        Ready: P9
         P17                               P12
                                           P10
----

If there are any processes in the max queue the scheduler will
pick these processes for execution. If there are no processes
in the max queue but there are processes in the high priority
queue the scheduler will pick those processes. Only if there
are no processes in the max and the high priority queues will
the scheduler pick the first process from the normal and low
queue.

When a normal process is inserted into the queue it gets a _schedule
count_ of 1 and a low priority process gets a schedule count of 8.
When a process is picked from the front of the
queue its schedule count is reduced by one, if the count reaches zero
the process is scheduled, otherwise it is inserted at the end of the
queue. This means that low priority processes will go through the
queue seven times before they are scheduled.

==== Waiting, Timeouts and the Timing Wheel

A processs trying to do a receive on an empty mailbox or on
a mailbox with no matching messages will yield and go into the
waiting state.

When a message is delivered to an inbox the sending process will check
whether the receiver is _sleeping_ in the waiting state, and in that
case it will _wake_ the process, change its state to runable, and put
it at the end of the appropriate ready queue.

If the receive statement has a +timeout+ clause a timer will be
created for the process which will trigger after the specified timeout
time. The only guarantee the runtime system gives on a timeout is that
it will not trigger before the set time, it might be some time after
the intended time before the process is scheduled and gets to execute.

Timers are handled in the VM by a _timing wheel_. That is, an array of
time slots which wraps around. Prior to Erlang 18 the timing wheel was
a global resource and there could be some contention for the write
lock if you had many processes inserting timers into the wheel. Make
sure you are using a later version of Erlang if you use many timers.

The default size (+TIW_SIZE+) of the timing wheel is 65536 slots (or
8192 slots if you have built the system for a small memory
footprint). The current time is indicated by an index into the array
(+tiw_pos+). When a timer is inserted into the wheel with a timeout of
T the timer is inserted into the slot at +(tiw_pos+T)%TIW_SIZE+.

[[the_timing_wheel]]
----

   0 1                                      65535
  +-+-+- ... +-+-+-+-+-+-+-+-+-+-+-+ ... +-+-----+
  | | |      | | | | | | |t| | | | |     | |     |
  +-+-+- ... +-+-+-+-+-+-+-+-+-+-+-+ ... +-+-----+
              ^           ^                       ^
              |           |                       |
           tiw_pos     tiw_pos+T               TIW_SIZE

----

The timer stored in the timing wheel is a pointer to an +ErlTimer+
struct. See link:https://github.com/erlang/otp/blob/OTP-19.1/erts/emulator/beam/erl_time.h[erl_time.h]. If several timers are
inserted into the same slot they are linked together in a linked list
by the +prev+ and +next+ fields. The +count+ field is set to 
+T/TIW_SIZE+ 


[[ErlTimer]]
[source,c]
----


/*
** Timer entry:
*/
typedef struct erl_timer {
    struct erl_timer* next;	/* next entry tiw slot or chain */
    struct erl_timer* prev;	/* prev entry tiw slot or chain */
    Uint slot;			/* slot in timer wheel */
    Uint count;			/* number of loops remaining */
    int    active;		/* 1=activated, 0=deactivated */
    /* called when timeout */
    void (*timeout)(void*);
    /* called when cancel (may be NULL) */
    void (*cancel)(void*);
    void* arg;        /* argument to timeout/cancel procs */
} ErlTimer;

----


=== Ports

A port is an Erlang abstraction for a communication point with the
world outside of the Erlang VM. Communications with sockets, pipes,
and file IO are all done through ports on the Erlang side.

A port, like a process, is created on the same scheduler as the
creating process. Also like processes ports use reductions to decide
when to yield, and they also get to run for 4000 reductions. But
since ports don't run Erlang code there are no Erlang function calls
to count as reductions, instead each _port task_ is counted as a
number of reductions. Currently a task uses a little more than 200
reductions per task, and a number of reductions relative to one
thousands of the size of transmitted data.

A port task is one operation on a port, like opening, closing, sending
a number of bytes or receiving data. In order to execute a port task
the executing thread takes a lock on the port.

Port tasks are scheduled and executed in each iteration in the
scheduler loop (see below) before a new process is selected for
execution.

=== Reductions

When a process is scheduled it will get a number of reductions defined
by `CONTEXT_REDS` (defined in
link:https://github.com/erlang/otp/blob/OTP-20.0/erts/emulator/beam/erl_vm.h[erl_vm.h],
currently as 4000). After using up its reductions or when doing a
up its reductions or when doing a receive without a matching message
in the inbox, the process will be suspended and a new processes will
be scheduled.

If the VM has executed as many reductions as defined by
`INPUT_REDUCTIONS` (currently `2*CONTEXT_REDS`, also defined in
+erl_vm.h+) or if there is no process ready to run the scheduler will
do system-level activities. That is, basically, check for IO; we will
cover the details soon.

It is not completely defined what a reduction is, but at least each
function call should be counted as a reduction. Things get a bit more
complicated when talking about BIFs and NIFs. A process should not be
able to run for "a long time" without using a reduction and yielding.
A function written in C can usually not yield at any time, and the
reason for writing it in C is usually to achieve performance. In such
functions a reduction might take longer which can lead to imbalance in
the scheduler.

For example in Erlang versions prior to R16 the BIFs
+binary_to_term/1+ and +term_to_binary/1+ where non yielding and only
counted as one reduction. This meant that a process calling theses
functions on large terms could starve other processes. This can even
happen in a SMP system because of the way processes are balanced
between schedulers, which we will get to soon.

While a process is running the emulator keeps the number of reductions
left to execute in the (register mapped) variable FCALLS (see
+beam_emu.c+).

// I have compiled a table of variable names used for reduction counting
// as a reference for you if you want to dive into the source code. In
// xref:redvars[] you can see the variables used globally and in the PCB
// and in the emulator and the scheduler.

// [[redvars]]
// [cols="1,2a"]
// |====
// | Global

// |

// [cols="1,3"]
// !====
// ! Variable ! Use

// ! +function_calls+ ! static (file global) variable in erl_process.c, number of function calls since last system-level activity

// !====

// | In PCB

// |

// [cols="1,3"]
// !====
// ! Variable                         ! Use
// ! p->fcalls                        !
// ! p->reds                          !
// ! REDS_IN == (+p->def_arg_reg[5]+) ! reds while swapped out?

// !====

// | beam_emu.c

// |

// [cols="1,3"]
// !====
// ! Variable   ! Use
// ! FCALLS     ! register mapped var for reductions
// ! reds_used  ! used reductions during execution, calls in erl_process.c schedule
// ! reds (c_p->fcalls) !
// ! neg_o_reds ! ("negative old value of reds when call saving is active")
// !====

// | erl_process.c schedule/2

// |

// [cols="1,3"]
// !====
// ! Variable         ! Use
// ! calls            ! argument to schedule
// ! context_reds     !
// ! fcalls           !
// ! input_reductions !
// ! actual_reds      !
// ! reds             !
// !====

// |====

=== 调度循环

Conceptually you can look at the scheduler as the driver of program
execution in the Erlang VM. In reality, that is, the way the C code
is structured, it is the emulator (+process_main+ in beam_emu.c) that
drives the execution and it calls the scheduler as a subroutine to find
the next process to execute.

Still, we will pretend that it is the other way around, since it makes
a nice conceptual model for the scheduler loop. That is, we see it
as the scheduler picking a process to execute and then handing over
the execution to the emulator.

Looking at it that way, the scheduler loop looks like this:

. Update reduction counters.
. Check timers
. If needed check balance
. If needed migrate processes and ports
. Do auxiliary scheduler work
. If needed check IO and update time
. While needed pick a port task to execute
. Pick a process to execute

// TODO: Expand on these bullets

=== 负载均衡

The current strategy of the load balancer is to use as few schedulers
as possible without overloading any CPU. The idea is that you will get
better performance through better memory locality when processes share
the same CPU.

One thing to note though is that the load balancing done in the
scheduler is between scheduler threads and not necessarily between
CPUs or cores. When you start the runtime system you can specify how
schedulers should be allocated to cores. The default behaviour is that
it is up to the OS to allocated scheduler threads to cores, but you
can also choose to bind schedulers to cores.

The load balancer assumes that there is one scheduler running on each
core so that moving a process from a overloaded scheduler to an under
utilized scheduler will give you more parallel processing power. If
you have changed how schedulers are allocated to cores, or if your OS
is overloaded or bad at assigning threads to cores, the load balancing
might actually work against you.

The load balancer uses two techniques to balance the load, _task
stealing_ and _migration_. Task stealing is used every time a
scheduler runs out of work, this technique will result in the work
becoming more spread out between schedulers. Migration is more
complicated and tries to compact the load to the right number of
schedulers.

==== Task Stealing
If a scheduler run queue is empty when it should pick a new process
to schedule the scheduler will try to steal work from another
scheduler.

First the scheduler takes a lock on itself to prevent other schedulers
to try to steal work from the current scheduler. Then it checks if
there are any inactive schedulers that it can steal a task from. If
there are no inactive schedulers with stealable tasks then it will
look at active schedulers, starting with schedulers having a higher id
than itself, trying to find a stealable task.

The task stealing will look at one scheduler at a time and try to
steal the highest priority task of that scheduler. Since this is done
per scheduler there might actually be higher priority tasks that are
stealable on another scheduler which will not be taken.

The task stealing tries to move tasks towards schedulers with lower
numbers by trying to steal from schedulers with higher numbers,
but since the stealing also will wrap around and steal from schedulers
with lower numbers the result is that processes are spread out on all
active schedulers.

Task stealing is quite fast and can be done on every iteration of
the scheduler loop when a scheduler has run out of tasks.

==== Migration

To really utilize the schedulers optimally a more elaborate migration
strategy is used. The current strategy is to compact the load to as
few schedulers as possible, while at the same time spread it out so
that no scheduler is overloaded.

This is done by the function _check_balance_ in _erl_process.c_.

The migration is done by first setting up a migration plan and then
letting schedulers execute on that plan until a new plan is set up.
Every 2000*CONTEXT_REDS reductions a scheduler calculates
a migration path per priority per scheduler by looking at the workload
of all schedulers. The migration path can have three different types of
values: 1) cleared 2) migrate to scheduler # 3) immigrate from
scheduler #

When a process becomes ready (for example by receiving a message or
triggering a timeout) it will normally be scheduled on the last
scheduler it ran on (S1). That is, if the migration path of that
scheduler (S1), at that priority, is cleared. If the migration path of
the scheduler is set to emigrate (to S2) the process will be handed over
to that scheduler if both S1 and S2 have unbalanced run-queues. We will
get back to what that means.

When a scheduler (S1) is to pick a new process to execute it checks to
see if it has an immigration path from (S2) set. If the two involved
schedulers have unbalanced run-queues S1 will steal a process from S2.

The migration path is calculated by comparing the maximum run-queues
for each scheduler for a certain priority. Each scheduler will update
a counter in each iteration of its scheduler loop keeping track of
the maximal queue length. This information is then used to calculate
an average (max) queue length (_AMQL_).

----
 Max
 Run Q
 Length
    5         o
              o
           o  o
Avg: 2.5 --------------
           o  o     o
    1      o  o     o

scheduler S1 S2 S3 S4
----

Then the schedulers are sorted on their max queue lengths.

----
 Max
 Run Q
 Length
    5               o
                    o
                 o  o
Avg: 2.5 --------------
              o  o  o
    1         o  o  o

scheduler S3 S4 S1 S2

           ^        ^
           |        |
          tix      fix
----

Any scheduler with a longer run queue than average (S1, S2) will be
marked for emigration and any scheduler with a shorter max run queue
than average (S3, S4) will be targeted for immigration.

This is done by looping over the ordered set of schedulers with two
indices (immigrate from (+fix+)) and (emigrate to (+tix+)). In each
iteration of the a loop the immigration path of S[tix] is set to S[fix]
and the emigration path of S[fix] is set to S[tix]. Then tix is increased
and fix decreased till they both pass the balance point. If one index
reaches the balance point first it wraps.

In the example:
 * Iteration 1: S2.emigrate_to = S3 and S3.immigrate_from = S2
 * Iteration 2: S1.emigrate_to = S4 and S4.immigrate_from = S1

Then we are done.

In reality things are a bit more complicated since schedulers can be
taken offline. The migration planning is only done for online
schedulers. Also, as mentioned before, this is done per priority
level.

When a process is to be inserted into a ready queue and there is a
migration path set from S1 to S2 the scheduler first checks that the
run queue of S1 is larger than AMQL and that the run queue of S2 is
smaller than the average. This way the migration is only allowed if
both queues are still unbalanced.

There are two exceptions though where a migration is forced even
when the queues are balanced or even imbalanced in the wrong way.
In both these cases a special evacuation flag is set which overrides
the balance test.

The evacuation flag is set when a scheduler is taken offline to
ensure that no new processes are scheduled on an offline scheduler.
The flag is also set when the scheduler detects that no progress is
made on some priority. That is, if there for example is a max priority
process which always is ready to run so that no normal priority processes
ever are scheduled. Then the evacuation flag will be set for the normal
priority queue for that scheduler.
